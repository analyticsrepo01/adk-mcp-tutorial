{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750647750.377580  840842 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    }
   ],
   "source": [
    "!pip freeze > requirements-frozen.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "The Model Context Protocol (MCP) is an open standard that simplifies how AI assistants connect with external data, tools, and systems. It achieves this by standardizing the way applications provide contextual information to Large Language Models (LLMs), creating a vital interface for models to interact directly with various external services.\n",
    "\n",
    "Developers building MCP-enabled applications have the flexibility to utilize existing third-party MCP servers or implement their own custom server solutions.\n",
    "\n",
    "This notebook focuses on the latter, demonstrating how to build custom MCP servers using Gemini 2.5 Pro. We will walk through code generation and testing for four specific examples:\n",
    "\n",
    "#### MCP server code generation:\n",
    "- Example 1: Building a BigQuery MCP Server\n",
    "- Example 2: Building a MedlinePlus MCP Server\n",
    "- Example 3: Building an NIH MCP Server\n",
    "- Exmaple 4: Building a Cocktail MCP Server\n",
    "\n",
    "#### MCP server code testing:\n",
    "- Option 1: Use LangChain MCP Adaptor (working in Jupyter Notebook only, not Colab)\n",
    "- Option 2: Build your own agent\n",
    "- Option 3: Use Google ADK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2seUlrSAUFFF"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Google Gen AI SDK and other required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tFy3H3aPgx12",
    "outputId": "b89d727c-dc9e-45cc-8cd7-2c689cbc818a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai google-cloud-secret-manager mcp geopy black google-cloud-bigquery langchain-mcp-adapters langchain langchain-google-vertexai langgraph google-adk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NyKGtVQjgx13"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GID9BJkUFFH"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YhCPdMrOUFFH"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import datetime\n",
    "import json\n",
    "import requests\n",
    "import black\n",
    "import re\n",
    "\n",
    "from typing import Union, Dict, List, Optional\n",
    "from google.genai.types import (\n",
    "    GenerateContentConfig,\n",
    ")\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.cloud import aiplatform\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from IPython.display import display, Markdown\n",
    "from langchain_core.messages import HumanMessage, ToolMessage, AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQLQ7GJ1UFFH"
   },
   "source": [
    "### Helper function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UOAvC-xpUFFH"
   },
   "outputs": [],
   "source": [
    "def get_url_content(url):\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Raise an exception if the request returned an error status code (like 404 or 500)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Get the content of the response as text (HTML, in this case)\n",
    "        # 'requests' automatically decodes the content based on HTTP headers\n",
    "        file_content = response.text\n",
    "\n",
    "        # Now you can work with the content\n",
    "        print(\"Successfully fetched content\")\n",
    "        return file_content\n",
    "        # Or, save it to a file:\n",
    "        # with open(\"server_page.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "        #     f.write(file_content)\n",
    "        # print(\"Content saved to server_page.html\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle potential errors during the request (e.g., network issues, DNS errors)\n",
    "        print(f\"Error fetching URL {url}: {e}\")\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        # Handle HTTP error responses (e.g., 404 Not Found, 503 Service Unavailable)\n",
    "        print(f\"HTTP Error for {url}: {e}\")\n",
    "\n",
    "\n",
    "def format_python(raw_code, output_filename):\n",
    "\n",
    "    try:\n",
    "        # Format the code string using black\n",
    "        # Use default FileMode which is generally recommended\n",
    "        formatted_code = black.format_str(raw_code, mode=black.FileMode())\n",
    "\n",
    "        # Save the formatted code to the specified file\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(formatted_code)\n",
    "\n",
    "        print(f\"Successfully formatted the code and saved it to '{output_filename}'\")\n",
    "\n",
    "    except black.InvalidInput as e:\n",
    "        print(\n",
    "            f\"Error formatting code: The input string does not seem to be valid Python syntax.\"\n",
    "        )\n",
    "        print(f\"Details: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing the file: {e}\")\n",
    "\n",
    "\n",
    "def extract_json_from_string(input_str: str) -> Optional[Union[Dict, List]]:\n",
    "    \"\"\"\n",
    "    Extracts JSON data from a string, handling potential variations.\n",
    "\n",
    "    This function attempts to find JSON data within a string. It specifically\n",
    "    looks for JSON enclosed in Markdown-like code fences (```json ... ```).\n",
    "    If such a block is found, it extracts and parses the content.\n",
    "    If no code block is found, it attempts to parse the entire input string\n",
    "    as JSON.\n",
    "\n",
    "    Args:\n",
    "        input_str: The string potentially containing JSON data. It might be\n",
    "                   a plain JSON string or contain a Markdown code block\n",
    "                   with JSON, possibly preceded by other text (like 'shame').\n",
    "\n",
    "    Returns:\n",
    "        The parsed JSON object (typically a dictionary or list) if valid\n",
    "        JSON is found and successfully parsed.\n",
    "        Returns None if no valid JSON is found, if parsing fails, or if the\n",
    "        input is not a string.\n",
    "    \"\"\"\n",
    "    if not isinstance(input_str, str):\n",
    "        # Handle cases where input is not a string\n",
    "        return None\n",
    "\n",
    "    # Pattern to find JSON within ```json ... ``` blocks\n",
    "    # - ````json`: Matches the start fence.\n",
    "    # - `\\s*`: Matches any leading whitespace after the fence marker.\n",
    "    # - `(.*?)`: Captures the content (non-greedily) between the fences. This is group 1.\n",
    "    # - `\\s*`: Matches any trailing whitespace before the end fence.\n",
    "    # - ` ``` `: Matches the end fence.\n",
    "    # - `re.DOTALL`: Allows '.' to match newline characters.\n",
    "    pattern = r\"```json\\s*(.*?)\\s*```\"\n",
    "    match = re.search(pattern, input_str, re.DOTALL)\n",
    "\n",
    "    json_string_to_parse = None\n",
    "\n",
    "    if match:\n",
    "        # If a markdown block is found, extract its content\n",
    "        json_string_to_parse = match.group(\n",
    "            1\n",
    "        ).strip()  # Get captured group and remove surrounding whitespace\n",
    "    else:\n",
    "        # If no markdown block, assume the *entire* input might be JSON\n",
    "        # We strip whitespace in case the string is just JSON with padding\n",
    "        json_string_to_parse = input_str.strip()\n",
    "\n",
    "    if not json_string_to_parse:\n",
    "        # If after stripping, the potential JSON string is empty, return None\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Attempt to parse the determined string (either from block or whole input)\n",
    "        parsed_json = json.loads(json_string_to_parse)\n",
    "        return parsed_json\n",
    "    except json.JSONDecodeError:\n",
    "        # Parsing failed, indicating the string wasn't valid JSON\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # Catch other potential unexpected errors during parsing\n",
    "        print(f\"An unexpected error occurred during JSON parsing: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def create_folder_if_not_exists(folder_path_str: str) -> bool:\n",
    "    \"\"\"\n",
    "    Creates a folder (and any necessary parent folders) if it doesn't already exist.\n",
    "    Uses print() for status and error messages.\n",
    "\n",
    "    Args:\n",
    "        folder_path_str (str): The path string for the folder to be created.\n",
    "                               Can be relative or absolute.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the folder already exists or was successfully created,\n",
    "              False if an error occurred during creation (e.g., permission denied).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert the string path to a Path object\n",
    "        folder_path = Path(folder_path_str)\n",
    "\n",
    "        # Use mkdir() with options:\n",
    "        # parents=True: Creates any necessary parent directories. Like 'mkdir -p'.\n",
    "        # exist_ok=True: Doesn't raise an error if the directory already exists.\n",
    "        folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Print confirmation (using resolve() to show the absolute path)\n",
    "        print(f\"Info: Successfully ensured folder exists: {folder_path.resolve()}\")\n",
    "        return True\n",
    "\n",
    "    except PermissionError:\n",
    "        print(\n",
    "            f\"Error: Permission denied: Could not create folder at '{folder_path_str}'.\"\n",
    "        )\n",
    "        return False\n",
    "    except OSError as e:\n",
    "        # Catch other OS-related errors (e.g., path is a file, invalid path format on Windows)\n",
    "        print(f\"Error: OS error creating folder '{folder_path_str}': {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        # Catch any other unexpected errors\n",
    "        print(\n",
    "            f\"Error: An unexpected error occurred creating folder '{folder_path_str}': {e}\"\n",
    "        )\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hi0XVQfPUFFI",
    "outputId": "c741cdb7-e063-410e-c07b-0f8dd92e9538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Successfully ensured folder exists: /home/jupyter/GenAI9/adk-mcp-tutorial/server\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_folder_if_not_exists(\"server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Option 1 use a  Vertex AI project\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Nqwi-5ufWp_B"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import JSON\n",
    "import json\n",
    "\n",
    "# Cloud project id.\n",
    "PROJECT_IDS = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_IDS[0]  # @param {type:\"string\"}\n",
    "\n",
    "if not PROJECT_ID:\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = \"us-central1\" # @param {type:\"string\"}\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"TRUE\" # Use Vertex AI API\n",
    "# [your-project-id]\n",
    "\n",
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hb7LxAKUFFJ"
   },
   "source": [
    "## Set up model id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9mLEZsp7UFFJ"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fS_i9-mUFFJ"
   },
   "source": [
    "### Get system instruction context info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IuZhsdeaUFFJ",
    "outputId": "aac3e58e-8a84-44f5-ae86-df37e234a845"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched content\n"
     ]
    }
   ],
   "source": [
    "# The URL you want to fetch\n",
    "url = \"https://modelcontextprotocol.io/quickstart/server\"\n",
    "reference_content = get_url_content(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lowOJ8A3UFFJ"
   },
   "source": [
    "### Set up system instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "S-0BEewiUFFJ"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ResponseSchema(BaseModel):\n",
    "    python_code: str\n",
    "    description: str\n",
    "\n",
    "\n",
    "system_instruction = f\"\"\"\n",
    "  You are an MCP server export.\n",
    "  Your mission is to write python code for MCP server.\n",
    "  Here's the MCP server development guide and example\n",
    "  {reference_content}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7I2xf5BUFFJ"
   },
   "source": [
    "#### Set function to generate MCP server code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "EK9r3GLOUFFJ"
   },
   "outputs": [],
   "source": [
    "def generate_mcp_server(prompt):\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_ID,\n",
    "        contents=prompt,\n",
    "        config=GenerateContentConfig(\n",
    "            system_instruction=system_instruction,\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=ResponseSchema,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TMNG7voUFFJ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZahhGTF1UFFJ"
   },
   "source": [
    "## Generate MCP Server Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "539ji0JKUFFJ"
   },
   "source": [
    "### Example 1:  Build MCP Server for Google Cloud BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wGJriNqvUFFJ"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "  Please create an MCP server code for google cloud big query. It has two tools. One is to list tables for all datasets, the other is to describe a table. Google cloud project id and location will be provided in the query string. please use project id to access BigQuery client.\n",
    "  Please output JSON output only.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ItD7ic-3UFFJ"
   },
   "outputs": [],
   "source": [
    "response_text = generate_mcp_server(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "smQlIEugUFFK",
    "outputId": "fd655442-7a60-4f81-9418-82be146dcde0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully formatted the code and saved it to 'server/bq.py'\n"
     ]
    }
   ],
   "source": [
    "python_code = extract_json_from_string(response_text)[\"python_code\"]\n",
    "format_python(python_code, \"server/bq.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxPujdUfUFFK"
   },
   "source": [
    "### Example 2:  Build MCP server for Medlineplus website\n",
    "Create an MCP server for\n",
    "https://medlineplus.gov/about/developers/webservices/ API service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "idoW9HWIUFFK",
    "outputId": "96e771bb-044f-4122-e43f-a0f6bd2b6f8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully formatted the code and saved it to 'server/med.py'\n"
     ]
    }
   ],
   "source": [
    "med_url = \"https://medlineplus.gov/about/developers/webservices/\"\n",
    "med_prompt_base = \"\"\"\n",
    "  Please create an MCP server code for https://medlineplus.gov/about/developers/webservices/. It has one tool, get_medical_term. You provide a medical term, this tool will return explanation of the medial term.\n",
    "\n",
    "  Here's the API details:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = [med_prompt_base, types.Part.from_uri(file_uri=med_url, mime_type=\"text/html\")]\n",
    "response_text = generate_mcp_server(prompt)\n",
    "python_code = extract_json_from_string(response_text)[\"python_code\"]\n",
    "\n",
    "format_python(python_code, \"server/med.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYkpsapcUFFK"
   },
   "source": [
    "### Example 3: Build MCP Server for NIH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04YCTw0aUFFK",
    "outputId": "569eefea-81ed-4252-9928-4bf11c1d4cdc"
   },
   "outputs": [],
   "source": [
    "nih_url = \"https://clinicaltables.nlm.nih.gov/apidoc/icd10cm/v3/doc.html\"\n",
    "nih_prompt_base = \"\"\"\n",
    "  Please create an MCP server code for NIH. It has one tool, get_icd_10_code. You provide a name or code, it will return top 5 results.\n",
    "\n",
    "  Here's the API details:\n",
    "\n",
    "\"\"\"\n",
    "prompt = [nih_prompt_base, types.Part.from_uri(file_uri=nih_url, mime_type=\"text/html\")]\n",
    "response_text = generate_mcp_server(prompt)\n",
    "python_code = extract_json_from_string(response_text)[\"python_code\"]\n",
    "\n",
    "format_python(python_code, \"server/nih.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfBkTRjlUFFK"
   },
   "source": [
    "### Example 4: Build MCP Server for the Cocktail DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ITPP5LbzUFFK",
    "outputId": "34c662b5-f77d-4287-ac46-5c539dab093a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully formatted the code and saved it to 'server/cocktail.py'\n"
     ]
    }
   ],
   "source": [
    "ct_url = \"https://www.thecocktaildb.com/api.php\"\n",
    "ct_prompt_base = \"\"\"\n",
    "  Please create an MCP server code for the cocktail db. It has 5 tools:\n",
    "  1. search cocktail by name\n",
    "  2. list all cocktail by first letter\n",
    "  3. search ingredient by name.\n",
    "  4. list random cocktails\n",
    "  5. lookup full cocktail details by id\n",
    "\n",
    "  Here's the API details:\n",
    "\n",
    "\"\"\"\n",
    "prompt = [ct_prompt_base, types.Part.from_uri(file_uri=ct_url, mime_type=\"text/html\")]\n",
    "response_text = generate_mcp_server(prompt)\n",
    "python_code = extract_json_from_string(response_text)[\"python_code\"]\n",
    "\n",
    "format_python(python_code, \"server/cocktail.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wl2JdSYYUFFN"
   },
   "source": [
    "## Testing MCP Servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkwDSYToUFFN"
   },
   "source": [
    "### Option 1: Use LangChain MCP adaptor to test MCP servers\n",
    "It works in Jupyter Notebook only, not working in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "XCEn9qaoUFFN"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "bFxcSQQoUFFN"
   },
   "outputs": [],
   "source": [
    "llm = ChatVertexAI(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    max_retries=6,\n",
    "    stop=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Czaf3CjiUFFN"
   },
   "outputs": [],
   "source": [
    "server_configs = {\n",
    "    \"nih\": {\n",
    "        \"command\": \"python\",\n",
    "        \"args\": [\"./server/nih.py\"],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "    \"med\": {\n",
    "        \"command\": \"python\",\n",
    "        \"args\": [\"./server/med.py\"],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "    \"bq\": {\n",
    "        \"command\": \"python\",\n",
    "        \"args\": [\"./server/bq.py\"],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "    \"cocktail\": {\n",
    "        \"command\": \"python\",\n",
    "        \"args\": [\"./server/cocktail.py\"],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAMoJqk8UFFN"
   },
   "source": [
    "Set up MCP Client using LangChain MCP Adaptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "RxI0stMfUFFN"
   },
   "outputs": [],
   "source": [
    "# async def run_lc_react_agent(server_configs, message):\n",
    "#     async with MultiServerMCPClient(server_configs) as client:\n",
    "#         agent = create_react_agent(llm, client.get_tools())\n",
    "\n",
    "#         agent_response = await agent.ainvoke({\"messages\": message})\n",
    "#         for response in agent_response[\"messages\"]:\n",
    "#             user = \"\"\n",
    "\n",
    "#             if isinstance(response, HumanMessage):\n",
    "#                 user = \"[User]\"\n",
    "#             elif isinstance(response, ToolMessage):\n",
    "#                 user = \"-Tool-\"\n",
    "#             elif isinstance(response, AIMessage):\n",
    "#                 user = \"[Agent]\"\n",
    "\n",
    "#             if isinstance(response.content, list):\n",
    "#                 display(Markdown(f'{user}: {response.content[0].get(\"text\", \"\")}'))\n",
    "#                 continue\n",
    "#             display(Markdown(f\"{user}: {response.content}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_lc_react_agent(server_configs, message):\n",
    "    client = MultiServerMCPClient(server_configs)\n",
    "    tools = await client.get_tools()\n",
    "    agent = create_react_agent(llm, tools)\n",
    "    \n",
    "    agent_response = await agent.ainvoke({\"messages\": message})\n",
    "    \n",
    "    for response in agent_response[\"messages\"]:\n",
    "        user = \"\"\n",
    "        if isinstance(response, HumanMessage):\n",
    "            user = \"[User]\"\n",
    "        elif isinstance(response, ToolMessage):\n",
    "            user = \"-Tool-\"\n",
    "        elif isinstance(response, AIMessage):\n",
    "            user = \"[Agent]\"\n",
    "        if isinstance(response.content, list):\n",
    "            display(Markdown(f'{user}: {response.content[0].get(\"text\", \"\")}'))\n",
    "            continue\n",
    "        display(Markdown(f\"{user}: {response.content}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qW0RPPM-UFFN"
   },
   "source": [
    "#### Example 1: BigQuery MCP server testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "uyzEGdZ2UFFN",
    "outputId": "30f7c964-5e59-4b5c-e8f7-facfa479007a"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[User]: Please list my bigquery tables for project 'my-project-0004-346516', the location is 'us', yes give me all the tables list"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "[Agent]: "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-Tool-: Dataset: census\n",
       "  - Table: census_data\n",
       "Dataset: copy_data_analyticsshareddata\n",
       "  - Table: testtable\n",
       "Dataset: darryl_video_rag_demo_lh\n",
       "  - Table: video_embeddings\n",
       "Dataset: data_analytics_shared_data\n",
       "  - Table: biglake_caspian_rls\n",
       "  - Table: bigsearch_log_5b_5t_json_hourly\n",
       "Dataset: formagent00\n",
       "  - Table: timeseries_data\n",
       "Dataset: gemini_bq_fn\n",
       "  - Table: books\n",
       "Dataset: model_deployment_monitoring_1911931973446664192\n",
       "  - Table: serving_predict\n",
       "Dataset: model_deployment_monitoring_7861011159342645248\n",
       "  - Table: serving_predict\n",
       "Dataset: movielens\n",
       "  - Table: movies\n",
       "  - Table: products\n",
       "  - Table: ratings\n",
       "  - Table: user_events\n",
       "Dataset: movielens_gcp_sa_retail\n",
       "  No tables found in this dataset.\n",
       "Dataset: my_project_0004_346516_unique\n",
       "  No tables found in this dataset.\n",
       "Dataset: vertex_feature_transform_engine_staging_us\n",
       "  No tables found in this dataset.\n",
       "Dataset: visionai_dataset\n",
       "  - Table: traffic-app"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "[Agent]: Of course, here is the list of tables for project 'my-project-0004-346516' in the 'us' location:\n",
       "\n",
       "**Dataset: census**\n",
       "* Table: census_data\n",
       "\n",
       "**Dataset: copy_data_analyticsshareddata**\n",
       "* Table: testtable\n",
       "\n",
       "**Dataset: darryl_video_rag_demo_lh**\n",
       "* Table: video_embeddings\n",
       "\n",
       "**Dataset: data_analytics_shared_data**\n",
       "* Table: biglake_caspian_rls\n",
       "* Table: bigsearch_log_5b_5t_json_hourly\n",
       "\n",
       "**Dataset: formagent00**\n",
       "* Table: timeseries_data\n",
       "\n",
       "**Dataset: gemini_bq_fn**\n",
       "* Table: books\n",
       "\n",
       "**Dataset: model_deployment_monitoring_1911931973446664192**\n",
       "* Table: serving_predict\n",
       "\n",
       "**Dataset: model_deployment_monitoring_7861011159342645248**\n",
       "* Table: serving_predict\n",
       "\n",
       "**Dataset: movielens**\n",
       "* Table: movies\n",
       "* Table: products\n",
       "* Table: ratings\n",
       "* Table: user_events\n",
       "\n",
       "**Dataset: visionai_dataset**\n",
       "* Table: traffic-app\n",
       "\n",
       "The following datasets were found but contain no tables:\n",
       "* movielens_gcp_sa_retail\n",
       "* my_project_0004_346516_unique\n",
       "* vertex_feature_transform_engine_staging_us"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await run_lc_react_agent(\n",
    "    server_configs,\n",
    "    \"Please list my bigquery tables for project 'my-project-0004-346516', the location is 'us', yes give me all the tables list\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbxmN3p_UFFN"
   },
   "source": [
    "#### Example 2: MedlinePlus MCP server testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "4XVHoVIuUFFN",
    "outputId": "43034b72-35cc-40c5-ace1-638810c1a3f0"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[User]: Please explain flu / Influenza in details"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "[Agent]: "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-Tool-: What is the <span class=\"qt0\"><span class=\"qt1\">flu</span></span>?<p>The <span class=\"qt0\"><span class=\"qt1\">flu</span></span>, also called <span class=\"qt0\"><span class=\"qt1\">influenza</span></span>, is a respiratory infection caused by viruses. Each year, millions of Americans get sick with the <span class=\"qt0\"><span class=\"qt1\">flu</span></span>. Sometimes it causes mild illness. But it can also be serious or even deadly, especially for people over 65, newborn babies, and people with certain chronic illnesses.</p>What causes the <span class=\"qt0\"><span class=\"qt1\">flu</span></span>?<p>The <span class=\"qt0\"><span class=\"qt1\">flu</span></span> is caused by <span class=\"qt0\"><span class=\"qt1\">flu</span></span> viruses that spread from person to person. When someone with the <span class=\"qt0\"><span class=\"qt1\">flu</span></span> coughs, sneezes, or talks, they spray tiny droplets. These droplets can land in the mouths or noses of people who are nearby. Less often, a person may get <span class=\"qt0\"><span class=\"qt1\">flu</span></span> by touching a surface or object that has <span class=\"qt0\"><span class=\"qt1\">flu</span></span> virus on it and then touching their own mouth, nose, or possibly their eyes.</p>What are the symptoms of the <span class=\"qt0\"><span class=\"qt1\">flu</span></span>?<p>Symptoms of the <span class=\"qt0\"><span class=\"qt1\">flu</span></span> come on suddenly and may include:</p><ul><li>Fever or feeling feverish/chills</li><li>Cough</li><li>Sore throat</li><li>Runny or stuffy nose</li><li>Muscle or body aches</li><li>Headaches</li><li>Fatigue (tiredness)</li></ul><p>Some people may also have vomiting and diarrhea. This is more common in children.</p><p>Sometimes people have trouble figuring out whether they have a cold or the <span class=\"qt0\"><span class=\"qt1\">flu</span></span>. There are differences between them:</p>Signs and SymptomsColdFluStart of symptomsSlowlySuddenlyFeverRarelyUsuallyAchesSometimes (slight)UsuallyFatigue, weaknessSometimesUsuallyHeadacheRarelyCommonStuffy nose, sneezing, or sore throatCommonSometimes<p>Sometimes people say that they have a <span class=\"qt0\"><span class=\"qt1\">\"flu\"</span></span> when they really have something else. For example, <span class=\"qt0\"><span class=\"qt1\">\"stomach flu\"</span></span> isn't the <span class=\"qt0\"><span class=\"qt1\">flu</span></span>; it's gastroenteritis.</p>What other problems can the <span class=\"qt0\"><span class=\"qt1\">flu</span></span> cause?<p>Some people who get the <span class=\"qt0\"><span class=\"qt1\">flu</span></span> will develop complications. Some of these complications can be serious or even life-threatening. They include:</p><ul><li>Bronchitis</li><li>Ear infection</li><li>Sinus infection</li><li>Pneumonia</li><li>Inflammation of the heart (myocarditis), brain (encephalitis), or muscle tissues (myositis, rhabdomyolysis)</li></ul><p>The <span class=\"qt0\"><span class=\"qt1\">flu</span></span> also can make chronic health problems worse. For example, people with asthma may have asthma attacks while they have <span class=\"qt0\"><span class=\"qt1\">flu</span></span>.</p><p>Certain people are more likely to have complications from the <span class=\"qt0\"><span class=\"qt1\">flu</span></span>, including:</p><ul><li>Adults 65 and older</li><li>Pregnant women</li><li>Children younger than 5</li><li>People with certain chronic health conditions, such as asthma, diabetes, and heart disease</li></ul>How is the <span class=\"qt0\"><span class=\"qt1\">flu</span></span> diagnosed?<p>To diagnose the <span class=\"qt0\"><span class=\"qt1\">flu</span></span>, health care providers will first do a medical history and ask about your symptoms. There are several tests for the <span class=\"qt0\"><span class=\"qt1\">flu</span></span>. For the tests, your provider will swipe the inside of your nose or the back of your throat with a swab. Then the swab will be tested for the <span class=\"qt0\"><span class=\"qt1\">flu</span></span> virus.</p><p>Some tests are quick and give results in 15-20 minutes. But these tests are not as accurate as other <span class=\"qt0\"><span class=\"qt1\">flu</span></span> tests. These other tests can give you the results in one hour or several hours.</p>What are the treatments for the <span class=\"qt0\"><span class=\"qt1\">flu</span></span>?<p>Most people with the <span class=\"qt0\"><span class=\"qt1\">flu</span></span> recover on their own without medical care. People with mild cases of the <span class=\"qt0\"><span class=\"qt1\">flu</span></span> should stay home and avoid contact with others, except to get medical care.</p><p>But if you have symptoms of <span class=\"qt0\"><span class=\"qt1\">flu</span></span> and are in a high risk group or are very sick or worried about your illness, contact your health care provider. You might need antiviral medicines to treat your <span class=\"qt0\"><span class=\"qt1\">flu</span></span>. Antiviral medicines can make the illness milder and shorten the time you are sick. They also can prevent serious <span class=\"qt0\"><span class=\"qt1\">flu</span></span> complications. They usually work best when you start taking them within 2 days of getting sick.</p>Can the <span class=\"qt0\"><span class=\"qt1\">flu</span></span> be prevented?<p>The best way to prevent the <span class=\"qt0\"><span class=\"qt1\">flu is to get a flu</span></span> vaccine every year. But it's also important to have good health habits like covering your cough and washing your hands often. This can help stop the spread of germs and prevent the <span class=\"qt0\"><span class=\"qt1\">flu</span></span>.</p><p>Centers for Disease Control and Prevention</p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "[Agent]: The flu, also known as influenza, is a respiratory infection caused by viruses. It's a common illness that affects millions of people each year. While it can be mild for some, it can also be serious and even life-threatening, particularly for vulnerable populations like the elderly, young children, and individuals with chronic health conditions.\n",
       "\n",
       "**Causes:**\n",
       "\n",
       "The flu is caused by influenza viruses that spread from person to person through respiratory droplets released when an infected person coughs, sneezes, or talks. You can also get the flu by touching a surface contaminated with the virus and then touching your mouth, nose, or eyes.\n",
       "\n",
       "**Symptoms:**\n",
       "\n",
       "Flu symptoms tend to come on suddenly and can include:\n",
       "\n",
       "*   Fever or feeling feverish/chills\n",
       "*   Cough\n",
       "*   Sore throat\n",
       "*   Runny or stuffy nose\n",
       "*   Muscle or body aches\n",
       "*   Headaches\n",
       "*   Fatigue (tiredness)\n",
       "*   Vomiting and diarrhea (more common in children)\n",
       "\n",
       "**Complications:**\n",
       "\n",
       "In some cases, the flu can lead to serious complications, such as:\n",
       "\n",
       "*   Bronchitis\n",
       "*   Ear and sinus infections\n",
       "*   Pneumonia\n",
       "*   Inflammation of the heart (myocarditis), brain (encephalitis), or muscle tissues (myositis, rhabdomyolysis)\n",
       "\n",
       "The flu can also worsen chronic health problems. For example, people with asthma may experience asthma attacks.\n",
       "\n",
       "**Diagnosis:**\n",
       "\n",
       "To diagnose the flu, healthcare providers will assess your symptoms and may use a flu test, which involves swabbing the inside of your nose or the back of your throat.\n",
       "\n",
       "**Treatment:**\n",
       "\n",
       "Most people with the flu recover on their own with rest and fluids. However, antiviral medications may be prescribed to those who are at high risk of complications or have a severe illness. These medications can help lessen the severity and duration of the illness.\n",
       "\n",
       "**Prevention:**\n",
       "\n",
       "The most effective way to prevent the flu is to get a flu vaccine each year. Good hygiene practices, such as frequent handwashing and covering your coughs and sneezes, are also important in preventing the spread of the virus."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await run_lc_react_agent(server_configs, \"Please explain flu / Influenza in details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwSOy5BYUFFN"
   },
   "source": [
    "#### Example 3: NIH MCP Server testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "PD3rA-7PUFFN",
    "outputId": "3c176b2f-317a-4571-d46c-4f60ba5f3317"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[User]: can you tell me icd-10 code for influenza A?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "[Agent]: "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-Tool-: No ICD-10 codes found for 'influenza A'."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "[Agent]: I am sorry, I cannot find the ICD-10 code for \"influenza A\". I will try searching for \"influenza\" instead."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-Tool-: No ICD-10 codes found for 'influenza'."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "[Agent]: I am sorry, I cannot find the ICD-10 code for \"influenza\" either. I will try searching for \"flu\" instead."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-Tool-: No ICD-10 codes found for 'flu'."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "[Agent]: I am sorry, I am unable to find the ICD-10 code for influenza A at the moment. Please try again later."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await run_lc_react_agent(server_configs, \"can you tell me icd-10 code for influenza A?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHFrBRVIUFFO"
   },
   "source": [
    "### Example 4: Cocktail MCP server testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "K-zvRCF9UFFO",
    "outputId": "5785c214-41f7-4c8c-b9d3-46b2dbd428ce"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[User]: Please get full detail of cocktail margarita"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "[Agent]: "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-Tool-: ID: 11007, Name: Margarita, Category: Ordinary Drink\n",
       "---\n",
       "ID: 11118, Name: Blue Margarita, Category: Ordinary Drink\n",
       "---\n",
       "ID: 17216, Name: Tommy's Margarita, Category: Ordinary Drink\n",
       "---\n",
       "ID: 16158, Name: Whitecap Margarita, Category: Other / Unknown\n",
       "---\n",
       "ID: 12322, Name: Strawberry Margarita, Category: Ordinary Drink\n",
       "---\n",
       "ID: 178332, Name: Smashed Watermelon Margarita, Category: Cocktail"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "[Agent]: Which margarita are you looking for? I have found a few options: Margarita, Blue Margarita, Tommy's Margarita, Whitecap Margarita, Strawberry Margarita, Smashed Watermelon Margarita."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await run_lc_react_agent(server_configs, \"Please get full detail of cocktail margarita\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xR7rwda7UFFO"
   },
   "source": [
    "#### Agent streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def run_streaming_agent():\n",
    "    client = MultiServerMCPClient(\n",
    "        {\n",
    "            \"nih\": {\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [\"./server/nih.py\"],\n",
    "                \"transport\": \"stdio\",\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    tools = await client.get_tools()\n",
    "    agent = create_react_agent(llm, tools)\n",
    "    \n",
    "    # Initialize conversation history using simple tuples\n",
    "    inputs = {\"messages\": []}\n",
    "    print(\"Agent is ready. Type 'exit' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Exiting chat.\")\n",
    "            break\n",
    "            \n",
    "        # Append user message to history\n",
    "        inputs[\"messages\"].append((\"user\", user_input))\n",
    "        \n",
    "        # call our graph with streaming to see the steps\n",
    "        async for state in agent.astream(inputs, stream_mode=\"values\"):\n",
    "            last_message = state[\"messages\"][-1]\n",
    "            print(last_message)\n",
    "            last_message.pretty_print()\n",
    "            \n",
    "        # update the inputs with the agent's response (fix: use = instead of ==)\n",
    "        inputs[\"messages\"] = state[\"messages\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "9QlgazALUFFO"
   },
   "outputs": [],
   "source": [
    "# async def run_streaming_agent():\n",
    "#     async with MultiServerMCPClient(\n",
    "#         {\n",
    "#             \"nih\": {\n",
    "#                 \"command\": \"python\",\n",
    "#                 \"args\": [\"./server/nih.py\"],\n",
    "#                 \"transport\": \"stdio\",\n",
    "#             },\n",
    "#         }\n",
    "#     ) as client:\n",
    "#         agent = create_react_agent(llm, client.get_tools())\n",
    "#         # Initialize conversation history using simple tuples\n",
    "#         inputs = {\"messages\": []}\n",
    "\n",
    "#         print(\"Agent is ready. Type 'exit' to quit.\")\n",
    "#         while True:\n",
    "#             user_input = input(\"You: \")\n",
    "#             if user_input.lower() == \"exit\":\n",
    "#                 print(\"Exiting chat.\")\n",
    "#                 break\n",
    "\n",
    "#             # Append user message to history\n",
    "#             inputs[\"messages\"].append((\"user\", user_input))\n",
    "\n",
    "#             # call our graph with streaming to see the steps\n",
    "#             async for state in agent.astream(inputs, stream_mode=\"values\"):\n",
    "#                 last_message = state[\"messages\"][-1]\n",
    "#                 print(last_message)\n",
    "#                 last_message.pretty_print()\n",
    "\n",
    "#             # update the inputs with the agent's response\n",
    "#             inputs[\"messages\"] == state[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2T5zUvmRUFFO",
    "outputId": "8af1ca7a-0273-486b-bb2f-c8647c454598"
   },
   "outputs": [],
   "source": [
    "# await run_streaming_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygUGnwCaUFFO"
   },
   "source": [
    "### Option 2: Build your own agent to test MCP servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9ySIvIsUFFO"
   },
   "source": [
    "\n",
    "##### Gemini Agent\n",
    "\n",
    "Within an MCP client session, this agent loop runs a multi-turn conversation loop with a Gemini model, handling tool calls via MCP server.\n",
    "\n",
    "This function orchestrates the interaction between a user prompt, a Gemini model capable of function calling, and a session object that provides and executes tools. It handles the cycle of:\n",
    "-  Gemini gets tool information from MCP client session\n",
    "-  Sending the user prompt (and conversation history) to the model.\n",
    "-  If the model requests tool calls, Gemini makes initial function calls to get structured data as per schema, and\n",
    "-  Sending the tool execution results back to the model.\n",
    "-  Repeating until the model provides a text response or the maximum number of tool execution turns is reached.\n",
    "-  Gemini generates final response based on tool responses and original query.\n",
    "  \n",
    "MCP integration with Gemini\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/github-repo/generative-ai/gemini/mcp/mcp_tool_call.png\" alt=\"MCP with Gemini\" height=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "zOEE6eXGUFFO"
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Consider using a more recent/recommended model if available and suitable\n",
    "\n",
    "DEFAULT_MAX_TOOL_TURNS = 5  # Maximum consecutive turns for tool execution\n",
    "DEFAULT_INITIAL_TEMPERATURE = (\n",
    "    0.0  # Temperature for the first LLM call (more deterministic)\n",
    ")\n",
    "DEFAULT_TOOL_CALL_TEMPERATURE = (\n",
    "    1.0  # Temperature for LLM calls after tool use (potentially more creative)\n",
    ")\n",
    "\n",
    "# Make tool calls via MCP Server\n",
    "\n",
    "\n",
    "async def _execute_tool_calls(\n",
    "    function_calls: List[types.FunctionCall], session: ClientSession\n",
    ") -> List[types.Part]:\n",
    "    \"\"\"\n",
    "    Executes a list of function calls requested by the Gemini model via the session.\n",
    "\n",
    "    Args:\n",
    "        function_calls: A list of FunctionCall objects from the model's response.\n",
    "        session: The session object capable of executing tools via `call_tool`.\n",
    "\n",
    "    Returns:\n",
    "        A list of Part objects, each containing a FunctionResponse corresponding\n",
    "        to the execution result of a requested tool call.\n",
    "    \"\"\"\n",
    "    tool_response_parts: List[types.Part] = []\n",
    "    print(f\"--- Executing {len(function_calls)} tool call(s) ---\")\n",
    "\n",
    "    for func_call in function_calls:\n",
    "        tool_name = func_call.name\n",
    "        # Ensure args is a dictionary, even if missing or not a dict type\n",
    "        args = func_call.args if isinstance(func_call.args, dict) else {}\n",
    "        print(f\"  Attempting to call session tool: '{tool_name}' with args: {args}\")\n",
    "\n",
    "        tool_result_payload: Dict[str, Any]\n",
    "        try:\n",
    "            # Execute the tool using the provided session object\n",
    "            # Assumes session.call_tool returns an object with attributes\n",
    "            # like `isError` (bool) and `content` (list of Part-like objects).\n",
    "            tool_result = await session.call_tool(tool_name, args)\n",
    "            print(f\"  Session tool '{tool_name}' execution finished.\")\n",
    "\n",
    "            # Extract result or error message from the tool result object\n",
    "            result_text = \"\"\n",
    "            # Check structure carefully based on actual `session.call_tool` return type\n",
    "            if (\n",
    "                hasattr(tool_result, \"content\")\n",
    "                and tool_result.content\n",
    "                and hasattr(tool_result.content[0], \"text\")\n",
    "            ):\n",
    "                result_text = tool_result.content[0].text or \"\"\n",
    "\n",
    "            if hasattr(tool_result, \"isError\") and tool_result.isError:\n",
    "                error_message = (\n",
    "                    result_text\n",
    "                    or f\"Tool '{tool_name}' failed without specific error message.\"\n",
    "                )\n",
    "                print(f\"  Tool '{tool_name}' reported an error: {error_message}\")\n",
    "                tool_result_payload = {\"error\": error_message}\n",
    "            else:\n",
    "                print(\n",
    "                    f\"  Tool '{tool_name}' succeeded. Result snippet: {result_text[:150]}...\"\n",
    "                )  # Log snippet\n",
    "                tool_result_payload = {\"result\": result_text}\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catch exceptions during the tool call itself\n",
    "            error_message = f\"Tool execution framework failed: {type(e).__name__}: {e}\"\n",
    "            print(f\"  Error executing tool '{tool_name}': {error_message}\")\n",
    "            tool_result_payload = {\"error\": error_message}\n",
    "\n",
    "        # Create a FunctionResponse Part to send back to the model\n",
    "        tool_response_parts.append(\n",
    "            types.Part.from_function_response(\n",
    "                name=tool_name, response=tool_result_payload\n",
    "            )\n",
    "        )\n",
    "    print(f\"--- Finished executing tool call(s) ---\")\n",
    "    return tool_response_parts\n",
    "\n",
    "\n",
    "async def run_agent_loop(\n",
    "    prompt: str,\n",
    "    client: genai.Client,\n",
    "    session: ClientSession,\n",
    "    model_id: str = MODEL_ID,\n",
    "    max_tool_turns: int = DEFAULT_MAX_TOOL_TURNS,\n",
    "    initial_temperature: float = DEFAULT_INITIAL_TEMPERATURE,\n",
    "    tool_call_temperature: float = DEFAULT_TOOL_CALL_TEMPERATURE,\n",
    ") -> types.GenerateContentResponse:\n",
    "    \"\"\"\n",
    "    Runs a multi-turn conversation loop with a Gemini model, handling tool calls.\n",
    "\n",
    "    This function orchestrates the interaction between a user prompt, a Gemini\n",
    "    model capable of function calling, and a session object that provides\n",
    "    and executes tools. It handles the cycle of:\n",
    "    1. Sending the user prompt (and conversation history) to the model.\n",
    "    2. If the model requests tool calls, executing them via the `session`.\n",
    "    3. Sending the tool execution results back to the model.\n",
    "    4. Repeating until the model provides a text response or the maximum\n",
    "       number of tool execution turns is reached.\n",
    "\n",
    "    Args:\n",
    "        prompt: The initial user prompt to start the conversation.\n",
    "        client: An initialized Gemini GenerativeModel client object\n",
    "\n",
    "        session: An active session object responsible for listing available tools\n",
    "                 via `list_tools()` and executing them via `call_tool(tool_name, args)`.\n",
    "                 It's also expected to have an `initialize()` method.\n",
    "        model_id: The identifier of the Gemini model to use (e.g., \"gemini-1.5-pro-latest\").\n",
    "        max_tool_turns: The maximum number of consecutive turns dedicated to tool calls\n",
    "                        before forcing a final response or exiting.\n",
    "        initial_temperature: The temperature setting for the first model call.\n",
    "        tool_call_temperature: The temperature setting for subsequent model calls\n",
    "                               that occur after tool execution.\n",
    "\n",
    "    Returns:\n",
    "        The final Response from the Gemini model after the\n",
    "        conversation loop concludes (either with a text response or after\n",
    "        reaching the max tool turns).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the session object does not provide any tools.\n",
    "        Exception: Can potentially raise exceptions from the underlying API calls\n",
    "                   or session tool execution if not caught internally by `_execute_tool_calls`.\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"Starting agent loop with model '{model_id}' and prompt: '{prompt[:100]}...'\"\n",
    "    )\n",
    "\n",
    "    # Initialize conversation history with the user's prompt\n",
    "    contents: List[types.Content] = [\n",
    "        types.Content(role=\"user\", parts=[types.Part(text=prompt)])\n",
    "    ]\n",
    "\n",
    "    # Ensure the session is ready (if needed)\n",
    "    if hasattr(session, \"initialize\") and callable(session.initialize):\n",
    "        print(\"Initializing session...\")\n",
    "        await session.initialize()\n",
    "    else:\n",
    "        print(\"Session object does not have an initialize() method, proceeding anyway.\")\n",
    "\n",
    "    # --- 1. Discover Tools from Session ---\n",
    "    print(\"Listing tools from session...\")\n",
    "    # Assumes session.list_tools() returns an object with a 'tools' attribute (list)\n",
    "    # Each item in the list should have 'name', 'description', and 'inputSchema' attributes.\n",
    "    session_tool_list = await session.list_tools()\n",
    "\n",
    "    if not session_tool_list or not session_tool_list.tools:\n",
    "        raise ValueError(\"No tools provided by the session. Agent loop cannot proceed.\")\n",
    "\n",
    "    # Convert session tools to the format required by the Gemini API\n",
    "    gemini_tool_config = types.Tool(\n",
    "        function_declarations=[\n",
    "            types.FunctionDeclaration(\n",
    "                name=tool.name,\n",
    "                description=tool.description,\n",
    "                parameters=tool.inputSchema,  # Assumes inputSchema is compatible\n",
    "            )\n",
    "            for tool in session_tool_list.tools\n",
    "        ]\n",
    "    )\n",
    "    print(\n",
    "        f\"Configured Gemini with {len(gemini_tool_config.function_declarations)} tool(s).\"\n",
    "    )\n",
    "\n",
    "    # --- 2. Initial Model Call ---\n",
    "    print(\"Making initial call to Gemini model...\")\n",
    "    current_temperature = initial_temperature\n",
    "    response = await client.aio.models.generate_content(\n",
    "        model=MODEL_ID,\n",
    "        contents=contents,  # Send updated history\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=1.0,\n",
    "            tools=[gemini_tool_config],\n",
    "        ),  # Keep sending same config\n",
    "    )\n",
    "    print(\"Initial response received.\")\n",
    "\n",
    "    # Append the model's first response (potentially including function calls) to history\n",
    "    # Need to handle potential lack of candidates or content\n",
    "    if not response.candidates:\n",
    "        print(\"Warning: Initial model response has no candidates.\")\n",
    "        # Decide how to handle this - raise error or return the empty response?\n",
    "        return response\n",
    "    contents.append(response.candidates[0].content)\n",
    "\n",
    "    # --- 3. Tool Calling Loop ---\n",
    "    turn_count = 0\n",
    "    # Check specifically for FunctionCall objects in the latest response part\n",
    "    latest_content = response.candidates[0].content\n",
    "    has_function_calls = any(part.function_call for part in latest_content.parts)\n",
    "\n",
    "    while has_function_calls and turn_count < max_tool_turns:\n",
    "        turn_count += 1\n",
    "        print(f\"\\n--- Tool Turn {turn_count}/{max_tool_turns} ---\")\n",
    "\n",
    "        # --- 3.1 Execute Pending Function Calls ---\n",
    "        function_calls_to_execute = [\n",
    "            part.function_call for part in latest_content.parts if part.function_call\n",
    "        ]\n",
    "        tool_response_parts = await _execute_tool_calls(\n",
    "            function_calls_to_execute, session\n",
    "        )\n",
    "\n",
    "        # --- 3.2 Add Tool Responses to History ---\n",
    "        # Send back the results for *all* function calls from the previous turn\n",
    "        contents.append(\n",
    "            types.Content(role=\"function\", parts=tool_response_parts)\n",
    "        )  # Use \"function\" role\n",
    "        print(f\"Added {len(tool_response_parts)} tool response part(s) to history.\")\n",
    "\n",
    "        # --- 3.3 Make Subsequent Model Call with Tool Responses ---\n",
    "        print(\"Making subsequent API call to Gemini with tool responses...\")\n",
    "        current_temperature = tool_call_temperature  # Use different temp for follow-up\n",
    "        response = await client.aio.models.generate_content(\n",
    "            model=MODEL_ID,\n",
    "            contents=contents,  # Send updated history\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=1.0,\n",
    "                tools=[gemini_tool_config],\n",
    "            ),\n",
    "        )\n",
    "        print(\"Subsequent response received.\")\n",
    "\n",
    "        # --- 3.4 Append latest model response and check for more calls ---\n",
    "        if not response.candidates:\n",
    "            print(\"Warning: Subsequent model response has no candidates.\")\n",
    "            break  # Exit loop if no candidates are returned\n",
    "        latest_content = response.candidates[0].content\n",
    "        contents.append(latest_content)\n",
    "        has_function_calls = any(part.function_call for part in latest_content.parts)\n",
    "        if not has_function_calls:\n",
    "            print(\n",
    "                \"Model response contains text, no further tool calls requested this turn.\"\n",
    "            )\n",
    "\n",
    "    # --- 4. Loop Termination Check ---\n",
    "    if turn_count >= max_tool_turns and has_function_calls:\n",
    "        print(\n",
    "            f\"Maximum tool turns ({max_tool_turns}) reached. Exiting loop even though function calls might be pending.\"\n",
    "        )\n",
    "    elif not has_function_calls:\n",
    "        print(\"Tool calling loop finished naturally (model provided text response).\")\n",
    "\n",
    "    # --- 5. Return Final Response ---\n",
    "    print(\"Agent loop finished. Returning final response.\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mq4yIxpfUFFO"
   },
   "source": [
    "#### Set up MCP client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "xMchcMQeUFFO"
   },
   "outputs": [],
   "source": [
    "async def run_simple_agent(server_params, query):\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(\n",
    "            read,\n",
    "            write,\n",
    "        ) as session:\n",
    "            # Test prompt\n",
    "            prompt = query\n",
    "            print(f\"Running agent loop with prompt: {prompt}\")\n",
    "            # Run agent loop\n",
    "            res = await run_agent_loop(prompt, client, session)\n",
    "            return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "VclZxud7UFFP"
   },
   "outputs": [],
   "source": [
    "bq_server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    # Make sure to update to the full absolute path to your server file\n",
    "    args=[\"./server/bq.py\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "wNSo3bH3UFFP"
   },
   "outputs": [],
   "source": [
    "med_server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    # Make sure to update to the full absolute path to your server file\n",
    "    args=[\"./server/med.py\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "cjpqcNqeUFFP"
   },
   "outputs": [],
   "source": [
    "nih_server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    # Make sure to update to the full absolute path to your server file\n",
    "    args=[\"./server/nih.py\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "aV713hkIUFFP"
   },
   "outputs": [],
   "source": [
    "ct_server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    # Make sure to update to the full absolute path to your server file\n",
    "    args=[\"./server/cocktail.py\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "JDcyJRa7UFFP",
    "outputId": "a4b6189a-a2f9-40e0-c005-a3c42f8eab5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running agent loop with prompt: Please list my BigQuery tables, project id is 'dw-genai-dev', location is 'us'\n",
      "Starting agent loop with model 'gemini-2.5-flash' and prompt: 'Please list my BigQuery tables, project id is 'dw-genai-dev', location is 'us'...'\n",
      "Initializing session...\n",
      "Listing tools from session...\n",
      "Configured Gemini with 2 tool(s).\n",
      "Making initial call to Gemini model...\n",
      "Initial response received.\n",
      "\n",
      "--- Tool Turn 1/5 ---\n",
      "--- Executing 1 tool call(s) ---\n",
      "  Attempting to call session tool: 'list_tables' with args: {'location': 'us', 'project_id': 'dw-genai-dev'}\n",
      "  Session tool 'list_tables' execution finished.\n",
      "  Tool 'list_tables' succeeded. Result snippet: No datasets found in project dw-genai-dev....\n",
      "--- Finished executing tool call(s) ---\n",
      "Added 1 tool response part(s) to history.\n",
      "Making subsequent API call to Gemini with tool responses...\n",
      "Subsequent response received.\n",
      "Model response contains text, no further tool calls requested this turn.\n",
      "Tool calling loop finished naturally (model provided text response).\n",
      "Agent loop finished. Returning final response.\n",
      "I can't find any datasets in the project 'dw-genai-dev'. Please check the project ID and try again.\n"
     ]
    }
   ],
   "source": [
    "bq_query = (\n",
    "    \"Please list my BigQuery tables, project id is 'dw-genai-dev', location is 'us'\"\n",
    ")\n",
    "bq_res = await run_simple_agent(bq_server_params, bq_query)\n",
    "print(bq_res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "wMr1U9o7UFFP",
    "outputId": "9259418d-3583-4679-e761-ce2d29ac25e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running agent loop with prompt: Please explain flu in detail.\n",
      "Starting agent loop with model 'gemini-2.5-flash' and prompt: 'Please explain flu in detail....'\n",
      "Initializing session...\n",
      "Listing tools from session...\n",
      "Configured Gemini with 1 tool(s).\n",
      "Making initial call to Gemini model...\n",
      "Initial response received.\n",
      "\n",
      "--- Tool Turn 1/5 ---\n",
      "--- Executing 1 tool call(s) ---\n",
      "  Attempting to call session tool: 'get_medical_term' with args: {'term': 'flu'}\n",
      "  Session tool 'get_medical_term' execution finished.\n",
      "  Tool 'get_medical_term' succeeded. Result snippet: What is the <span class=\"qt0\">flu</span>?<p>The <span class=\"qt0\">flu</span>, also called <span class=\"qt0\">influenza</span>, is a respiratory infecti...\n",
      "--- Finished executing tool call(s) ---\n",
      "Added 1 tool response part(s) to history.\n",
      "Making subsequent API call to Gemini with tool responses...\n",
      "Subsequent response received.\n",
      "Model response contains text, no further tool calls requested this turn.\n",
      "Tool calling loop finished naturally (model provided text response).\n",
      "Agent loop finished. Returning final response.\n",
      "The flu, also known as influenza, is a respiratory infection caused by viruses. Millions of Americans get sick with the flu each year. While it can cause mild illness, it can also be serious or even deadly, particularly for individuals over 65, newborn babies, and people with certain chronic illnesses.\n",
      "\n",
      "**Causes:**\n",
      "Flu viruses spread from person to person. When an infected individual coughs, sneezes, or talks, they release tiny droplets that can be inhaled by others nearby. Less commonly, the flu can be spread by touching a surface or object contaminated with the flu virus and then touching one's own mouth, nose, or eyes.\n",
      "\n",
      "**Symptoms:**\n",
      "Flu symptoms typically appear suddenly and may include:\n",
      "*   Fever or feeling feverish/chills\n",
      "*   Cough\n",
      "*   Sore throat\n",
      "*   Runny or stuffy nose\n",
      "*   Muscle or body aches\n",
      "*   Headaches\n",
      "*   Fatigue (tiredness)\n",
      "\n",
      "Some individuals, especially children, may also experience vomiting and diarrhea. It's important to note that \"stomach flu\" is not influenza; it's gastroenteritis.\n",
      "\n",
      "**Distinguishing Flu from a Cold:**\n",
      "\n",
      "| Sign/Symptom          | Cold            | Flu             |\n",
      "| :-------------------- | :-------------- | :-------------- |\n",
      "| Start of symptoms     | Slowly          | Suddenly        |\n",
      "| Fever                 | Rarely          | Usually         |\n",
      "| Aches                 | Sometimes (slight) | Usually         |\n",
      "| Fatigue, weakness     | Sometimes       | Usually         |\n",
      "| Headache              | Rarely          | Common          |\n",
      "| Stuffy nose, sneezing, or sore throat | Common          | Sometimes       |\n",
      "\n",
      "**Complications:**\n",
      "Some people who contract the flu can develop serious or even life-threatening complications, including:\n",
      "*   Bronchitis\n",
      "*   Ear infection\n",
      "*   Sinus infection\n",
      "*   Pneumonia\n",
      "*   Inflammation of the heart (myocarditis), brain (encephalitis), or muscle tissues (myositis, rhabdomyolysis)\n",
      "\n",
      "The flu can also worsen existing chronic health problems, such as asthma attacks in individuals with asthma.\n",
      "\n",
      "**Individuals more likely to have complications:**\n",
      "*   Adults 65 and older\n",
      "*   Pregnant women\n",
      "*   Children younger than 5\n",
      "*   People with certain chronic health conditions, such as asthma, diabetes, and heart disease\n",
      "\n",
      "**Diagnosis:**\n",
      "To diagnose the flu, healthcare providers will conduct a medical history and inquire about your symptoms. Flu tests involve swabbing the inside of the nose or the back of the throat to test for the flu virus. While some rapid tests provide results in 15-20 minutes, they are less accurate than other tests that provide results in one or several hours.\n",
      "\n",
      "**Treatment:**\n",
      "Most people with the flu recover on their own without medical care. Those with mild cases should stay home and avoid contact with others, except for medical care.\n",
      "\n",
      "However, if you have flu symptoms and are in a high-risk group or are very sick, contact your healthcare provider. Antiviral medicines may be prescribed to treat the flu, which can make the illness milder, shorten its duration, and prevent serious complications. These medicines are most effective when started within 2 days of getting sick.\n",
      "\n",
      "**Prevention:**\n",
      "The most effective way to prevent the flu is to get a flu vaccine every year. Additionally, practicing good health habits such as covering coughs and frequent handwashing can help prevent the spread of germs and the flu.\n"
     ]
    }
   ],
   "source": [
    "med_query = \"Please explain flu in detail.\"\n",
    "med_res = await run_simple_agent(med_server_params, med_query)\n",
    "print(med_res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "fBE2QqbaUFFP",
    "outputId": "2e66fbc8-a408-4776-d31d-7dcbce6506a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running agent loop with prompt: Please tell me icd-10 code for pneumonia\n",
      "Starting agent loop with model 'gemini-2.5-flash' and prompt: 'Please tell me icd-10 code for pneumonia...'\n",
      "Initializing session...\n",
      "Listing tools from session...\n",
      "Configured Gemini with 1 tool(s).\n",
      "Making initial call to Gemini model...\n",
      "Initial response received.\n",
      "\n",
      "--- Tool Turn 1/5 ---\n",
      "--- Executing 1 tool call(s) ---\n",
      "  Attempting to call session tool: 'get_icd_10_code' with args: {'name_or_code': 'pneumonia'}\n",
      "  Session tool 'get_icd_10_code' execution finished.\n",
      "  Tool 'get_icd_10_code' succeeded. Result snippet: No ICD-10 codes found for 'pneumonia'....\n",
      "--- Finished executing tool call(s) ---\n",
      "Added 1 tool response part(s) to history.\n",
      "Making subsequent API call to Gemini with tool responses...\n",
      "Subsequent response received.\n",
      "Model response contains text, no further tool calls requested this turn.\n",
      "Tool calling loop finished naturally (model provided text response).\n",
      "Agent loop finished. Returning final response.\n",
      "I cannot give you a single ICD-10 code for pneumonia, as there are many different types, each with its own code. Please provide a more specific type of pneumonia for a more accurate result.\n"
     ]
    }
   ],
   "source": [
    "nih_query = \"Please tell me icd-10 code for pneumonia\"\n",
    "nih_res = await run_simple_agent(nih_server_params, nih_query)\n",
    "print(nih_res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "_MdAg8xMUFFP",
    "outputId": "1c00ea66-d88b-4edc-93e0-9e09664d5c9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running agent loop with prompt: Please tell me the details of cocktail margarita\n",
      "Starting agent loop with model 'gemini-2.5-flash' and prompt: 'Please tell me the details of cocktail margarita...'\n",
      "Initializing session...\n",
      "Listing tools from session...\n",
      "Configured Gemini with 5 tool(s).\n",
      "Making initial call to Gemini model...\n",
      "Initial response received.\n",
      "\n",
      "--- Tool Turn 1/5 ---\n",
      "--- Executing 1 tool call(s) ---\n",
      "  Attempting to call session tool: 'search_cocktail_by_name' with args: {'name': 'margarita'}\n",
      "  Session tool 'search_cocktail_by_name' execution finished.\n",
      "  Tool 'search_cocktail_by_name' succeeded. Result snippet: ID: 11007, Name: Margarita, Category: Ordinary Drink\n",
      "---\n",
      "ID: 11118, Name: Blue Margarita, Category: Ordinary Drink\n",
      "---\n",
      "ID: 17216, Name: Tommy's Margar...\n",
      "--- Finished executing tool call(s) ---\n",
      "Added 1 tool response part(s) to history.\n",
      "Making subsequent API call to Gemini with tool responses...\n",
      "Subsequent response received.\n",
      "Model response contains text, no further tool calls requested this turn.\n",
      "Tool calling loop finished naturally (model provided text response).\n",
      "Agent loop finished. Returning final response.\n",
      "There are several Margarita cocktails. Could you please specify which one you are referring to?\n",
      "* Margarita\n",
      "* Blue Margarita\n",
      "* Tommy's Margarita\n",
      "* Whitecap Margarita\n",
      "* Strawberry Margarita\n",
      "* Smashed Watermelon Margarita\n"
     ]
    }
   ],
   "source": [
    "ct_query = \"Please tell me the details of cocktail margarita\"\n",
    "bq_res = await run_simple_agent(ct_server_params, ct_query)\n",
    "print(bq_res.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwVyaSIiUFFP"
   },
   "source": [
    "### Option 3 Testing with Google ADK\n",
    "Note: It works in  Jupyter Notebook only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U google-adk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "FEC_C7YpUFFP"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SseServerParams' from 'google.adk.tools.mcp_tool.mcp_toolset' (/opt/conda/lib/python3.10/site-packages/google/adk/tools/mcp_tool/mcp_toolset.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmcp_tool\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmcp_toolset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     MCPToolset,\n\u001b[1;32m      3\u001b[0m     SseServerParams,\n\u001b[1;32m      4\u001b[0m     StdioServerParameters,\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SseServerParams' from 'google.adk.tools.mcp_tool.mcp_toolset' (/opt/conda/lib/python3.10/site-packages/google/adk/tools/mcp_tool/mcp_toolset.py)"
     ]
    }
   ],
   "source": [
    "from google.adk.tools.mcp_tool.mcp_toolset import (\n",
    "    MCPToolset,\n",
    "    # SseServerParams,\n",
    "    StdioServerParameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sV2g4vL0UFFP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"1\"\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEaXtUOgUFFP"
   },
   "outputs": [],
   "source": [
    "from google.adk.tools.mcp_tool.mcp_toolset import (\n",
    "    MCPToolset,\n",
    "    # SseServerParams,\n",
    "    StdioServerParameters,\n",
    ")\n",
    "from google.adk.agents.llm_agent import LlmAgent\n",
    "\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.genai import types\n",
    "\n",
    "from google.adk.artifacts.in_memory_artifact_service import InMemoryArtifactService\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "async def get_tools_async(server_params):\n",
    "    \"\"\"Gets tools from MCP Server.\"\"\"\n",
    "    tools, exit_stack = await MCPToolset.from_server(connection_params=server_params)\n",
    "    # MCP requires maintaining a connection to the local MCP Server.\n",
    "    # Using exit_stack to clean up server connection before exit.\n",
    "    return tools, exit_stack\n",
    "\n",
    "\n",
    "async def get_agent_async(server_params):\n",
    "    \"\"\"Creates an ADK Agent with tools from MCP Server.\"\"\"\n",
    "    tools, exit_stack = await get_tools_async(server_params)\n",
    "    root_agent = LlmAgent(\n",
    "        model=MODEL_ID,\n",
    "        name=\"ai_assistant\",\n",
    "        instruction=\"Use tools to get information to answer user questions\",\n",
    "        tools=tools,\n",
    "    )\n",
    "    return root_agent, exit_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZLuYLajUFFP"
   },
   "source": [
    "### An Agent built using Google ADK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAXKTZ0dUFFP"
   },
   "outputs": [],
   "source": [
    "async def run_adk_agent(server_params, question):\n",
    "    session_service = InMemorySessionService()\n",
    "    artifacts_service = InMemoryArtifactService()\n",
    "    session = session_service.create_session(state={}, app_name=\"my_app\", user_id=\"123\")\n",
    "\n",
    "    query = question\n",
    "    print(\"[user]: \", query)\n",
    "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
    "    root_agent, exit_stack = await get_agent_async(server_params)\n",
    "    runner = Runner(\n",
    "        app_name=\"my_app\",\n",
    "        agent=root_agent,\n",
    "        artifact_service=artifacts_service,\n",
    "        session_service=session_service,\n",
    "    )\n",
    "    events_async = runner.run_async(\n",
    "        session_id=session.id, user_id=\"123\", new_message=content\n",
    "    )\n",
    "    # print(events_async)\n",
    "    response = {}\n",
    "    async for event in events_async:\n",
    "        # print(event)\n",
    "        if event.content.role == \"user\" and event.content.parts[0].text:\n",
    "            print(\"[user]:\", event.content.parts[0].text)\n",
    "            response[\"user\"] = event.content.parts[0].text\n",
    "        if event.content.parts[0].function_response:\n",
    "            print(\"[-tool_response-]\", event.content.parts[0].function_response)\n",
    "            response[\"-tool_response-\"] = event.content.parts[0].function_response\n",
    "        if event.content.role == \"model\" and event.content.parts[0].text:\n",
    "            print(\"[agent]:\", event.content.parts[0].text)\n",
    "            response[\"agent\"] = event.content.parts[0].text\n",
    "\n",
    "    await exit_stack.aclose()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2nY0YkqUFFP"
   },
   "outputs": [],
   "source": [
    "# For this server, make sure you have Node.js installed on your machine\n",
    "bnb_server_params = StdioServerParameters(\n",
    "    command=\"npx\", args=[\"-y\", \"@openbnb/mcp-server-airbnb\", \"--ignore-robots-txt\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttMDGDekUFFQ",
    "outputId": "102f6e1b-0821-4bf8-eecc-5da4e95b6d94"
   },
   "outputs": [],
   "source": [
    "response = await run_adk_agent(\n",
    "    bnb_server_params,\n",
    "    \"Please find a room in LA, CA, April 15, 2025, checkout date is april 18, 2 adults\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "USm04QF0UFFQ",
    "outputId": "fbb81aa1-0e42-4efd-a068-9962d5ac0c7a"
   },
   "outputs": [],
   "source": [
    "await run_adk_agent(bnb_server_params, \"give more details of id 7462294\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhN6ZYX2UFFQ",
    "outputId": "e72cbd82-8d0c-46b9-d4c6-572665cc7bc5"
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Exiting chat.\")\n",
    "        break\n",
    "    await run_adk_agent(bnb_server_params, user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbjuvMrGUFFQ"
   },
   "outputs": [],
   "source": [
    "ct_server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[\"./server/cocktail2.py\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkfvcCdNUFFQ",
    "outputId": "267bdc39-9f61-496a-eb8f-2dbf88803d46"
   },
   "outputs": [],
   "source": [
    "await run_adk_agent(\n",
    "    ct_server_params,\n",
    "    \"Please get cocktail margarita id and then full detail of cocktail margarita\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYSS8SYfUFFQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrbzdLoLUFFQ"
   },
   "source": [
    "### References:\n",
    "\n",
    "https://modelcontextprotocol.io/introduction  \n",
    "\n",
    "https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-mcp-example.ipynb  \n",
    "\n",
    "https://github.com/modelcontextprotocol/python-sdk"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py312",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "py312 (Local)",
   "language": "python",
   "name": "conda-base-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
